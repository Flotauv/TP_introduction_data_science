{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86de4351-2b8c-40a2-a12e-64f870479d63",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "10b5deba-1b2d-401f-927f-07efdfc9eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fc89c4d6-3d0d-484a-bcaf-328d33c57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objets = pd.read_csv('objets-trouves-restitution.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83800a1c-d248-4bf6-b5f0-9861bcc4da2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Date et heure de restitution</th>\n",
       "      <th>Gare</th>\n",
       "      <th>Code UIC</th>\n",
       "      <th>Nature d'objets</th>\n",
       "      <th>Type d'objets</th>\n",
       "      <th>Type d'enregistrement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-16T08:46:55+02:00</td>\n",
       "      <td>2018-08-16T10:52:08+02:00</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>87481002.0</td>\n",
       "      <td>Téléphone portable</td>\n",
       "      <td>Appareils électroniques, informatiques, appare...</td>\n",
       "      <td>Objet trouvé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-16T08:50:34+02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Redon</td>\n",
       "      <td>87471300.0</td>\n",
       "      <td>Casque (vélo, moto)</td>\n",
       "      <td>Vélos, trottinettes, accessoires 2 roues</td>\n",
       "      <td>Objet trouvé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-16T09:11:18+02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Béziers</td>\n",
       "      <td>87781005.0</td>\n",
       "      <td>Sac d'enseigne (plastique, papier, …)</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>Objet trouvé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-16T09:19:47+02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rennes</td>\n",
       "      <td>87471003.0</td>\n",
       "      <td>Manteau, veste, blazer, parka, blouson, cape</td>\n",
       "      <td>Vêtements, chaussures</td>\n",
       "      <td>Objet trouvé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-16T09:23:18+02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris Montparnasse</td>\n",
       "      <td>87391003.0</td>\n",
       "      <td>Sac à dos</td>\n",
       "      <td>Bagagerie: sacs, valises, cartables</td>\n",
       "      <td>Objet trouvé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date Date et heure de restitution                Gare  \\\n",
       "0  2018-08-16T08:46:55+02:00    2018-08-16T10:52:08+02:00              Nantes   \n",
       "1  2018-08-16T08:50:34+02:00                          NaN               Redon   \n",
       "2  2018-08-16T09:11:18+02:00                          NaN             Béziers   \n",
       "3  2018-08-16T09:19:47+02:00                          NaN              Rennes   \n",
       "4  2018-08-16T09:23:18+02:00                          NaN  Paris Montparnasse   \n",
       "\n",
       "     Code UIC                               Nature d'objets  \\\n",
       "0  87481002.0                            Téléphone portable   \n",
       "1  87471300.0                           Casque (vélo, moto)   \n",
       "2  87781005.0         Sac d'enseigne (plastique, papier, …)   \n",
       "3  87471003.0  Manteau, veste, blazer, parka, blouson, cape   \n",
       "4  87391003.0                                     Sac à dos   \n",
       "\n",
       "                                       Type d'objets Type d'enregistrement  \n",
       "0  Appareils électroniques, informatiques, appare...          Objet trouvé  \n",
       "1           Vélos, trottinettes, accessoires 2 roues          Objet trouvé  \n",
       "2                Bagagerie: sacs, valises, cartables          Objet trouvé  \n",
       "3                              Vêtements, chaussures          Objet trouvé  \n",
       "4                Bagagerie: sacs, valises, cartables          Objet trouvé  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_objets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ce6e970f-e747-41c4-9d57-d38355eff218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de doublons : 80\n"
     ]
    }
   ],
   "source": [
    "doublons = df_objets.duplicated().sum()\n",
    "\n",
    "# Affiche le nombre de doublons\n",
    "print(f\"Nombre de doublons : {doublons}\")\n",
    "\n",
    "f_objets = df_objets.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33b30b70-f5e9-46a8-b410-99414f642284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 907276 entries, 0 to 907275\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   Date                          907276 non-null  object \n",
      " 1   Date et heure de restitution  350898 non-null  object \n",
      " 2   Gare                          906197 non-null  object \n",
      " 3   Code UIC                      906197 non-null  float64\n",
      " 4   Nature d'objets               907276 non-null  object \n",
      " 5   Type d'objets                 907276 non-null  object \n",
      " 6   Type d'enregistrement         907276 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 48.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_objets.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1778477-8d16-4c8f-96d6-9b8dccc53e6e",
   "metadata": {},
   "source": [
    "Il y a **90725 observations**, **7 colonnes** dont seul **la colonne Code UIC a pour type un float, les autres étant du texte**.\n",
    "\n",
    "On remarque également que pour toutes les colonnes, le nombre non null de valeur n'est pas toujours égal au nombre d'observations. Donc **il manque des valeurs dans les colonnes Code UIC, Gare, et Date et jeure de restitution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ca22fb28-16b9-4675-b754-aa26119e05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Date Date et heure de restitution  \\\n",
      "count                      907276                       350898   \n",
      "unique                     904783                       350213   \n",
      "top     2017-07-04T07:29:58+02:00    2023-09-27T02:00:22+02:00   \n",
      "freq                            4                           42   \n",
      "\n",
      "                      Gare              Nature d'objets  \\\n",
      "count               906197                       907276   \n",
      "unique                 181                          132   \n",
      "top     Paris Gare de Lyon  Porte-monnaie, portefeuille   \n",
      "freq                 70984                        82184   \n",
      "\n",
      "                              Type d'objets Type d'enregistrement  \n",
      "count                                907276                907276  \n",
      "unique                                   16                     1  \n",
      "top     Bagagerie: sacs, valises, cartables          Objet trouvé  \n",
      "freq                                 289118                907276  \n"
     ]
    }
   ],
   "source": [
    "print(df_objets.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "770230e8-c1e3-4141-b675-4be9781ebeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifier \"Type d'enregistrement\" en \"Objet restitué\" si \"Date et heure de restitution\" n'est pas NaN\n",
    "df_objets['Type d\\'enregistrement'] = np.where(\n",
    "    df_objets['Date et heure de restitution'].notna(),\n",
    "    'Objet restitué',\n",
    "    df_objets['Type d\\'enregistrement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6e6ea67-4a19-458e-9c28-483266db37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaque Gare a un unique Code UIC.\n",
      "Chaque Code UIC correspond à une unique Gare.\n"
     ]
    }
   ],
   "source": [
    "# Vérifie si chaque Gare a un unique Code UIC\n",
    "gare_unique_uic = df_objets.groupby(\"Gare\")[\"Code UIC\"].nunique()\n",
    "gare_incorrecte = gare_unique_uic[gare_unique_uic > 1]\n",
    "\n",
    "# Vérifie si chaque Code UIC correspond à une unique Gare\n",
    "uic_unique_gare = df_objets.groupby(\"Code UIC\")[\"Gare\"].nunique()\n",
    "uic_incorrect = uic_unique_gare[uic_unique_gare > 1]\n",
    "\n",
    "# Affiche les résultats\n",
    "if not gare_incorrecte.empty:\n",
    "    print(\"Incohérences trouvées pour les Gares avec plusieurs Codes UIC:\")\n",
    "    print(gare_incorrecte)\n",
    "else:\n",
    "    print(\"Chaque Gare a un unique Code UIC.\")\n",
    "\n",
    "if not uic_incorrect.empty:\n",
    "    print(\"Incohérences trouvées pour les Codes UIC avec plusieurs Gares:\")\n",
    "    print(uic_incorrect)\n",
    "else:\n",
    "    print(\"Chaque Code UIC correspond à une unique Gare.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f73d1109-bd02-44a9-92b7-ee9a0f982d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Code UIC'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_objets \u001b[38;5;241m=\u001b[39m df_objets\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode UIC\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m df_objets\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Code UIC'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_objets = df_objets.drop(columns=[\"Code UIC\"])\n",
    "df_objets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4575aff2-7aad-4709-948b-3b19c9ef581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date Date et heure de restitution                Gare  \\\n",
      "0  2018-08-16T08:46:55+02:00    2018-08-16T10:52:08+02:00              Nantes   \n",
      "1  2018-08-16T08:50:34+02:00                          NaN               Redon   \n",
      "2  2018-08-16T09:11:18+02:00                          NaN             Béziers   \n",
      "3  2018-08-16T09:19:47+02:00                          NaN              Rennes   \n",
      "4  2018-08-16T09:23:18+02:00                          NaN  Paris Montparnasse   \n",
      "\n",
      "                                Nature d'objets  \\\n",
      "0                            Téléphone portable   \n",
      "1                           Casque (vélo, moto)   \n",
      "2         Sac d'enseigne (plastique, papier, …)   \n",
      "3  Manteau, veste, blazer, parka, blouson, cape   \n",
      "4                                     Sac à dos   \n",
      "\n",
      "                                       Type d'objets Type d'enregistrement  \\\n",
      "0  Appareils électroniques, informatiques, appare...        Objet restitué   \n",
      "1           Vélos, trottinettes, accessoires 2 roues          Objet trouvé   \n",
      "2                Bagagerie: sacs, valises, cartables          Objet trouvé   \n",
      "3                              Vêtements, chaussures          Objet trouvé   \n",
      "4                Bagagerie: sacs, valises, cartables          Objet trouvé   \n",
      "\n",
      "  Année Mois Jour     Heure Décalage_Horaire  \n",
      "0  2018   08   16  08:46:55           +02:00  \n",
      "1  2018   08   16  08:50:34           +02:00  \n",
      "2  2018   08   16  09:11:18           +02:00  \n",
      "3  2018   08   16  09:19:47           +02:00  \n",
      "4  2018   08   16  09:23:18           +02:00  \n"
     ]
    }
   ],
   "source": [
    "# Extraire l'année, le mois, le jour et le décalage horaire en utilisant des sous-chaînes de 'Date'\n",
    "df_objets['Année'] = df_objets['Date'].str[:4]         # Les 4 premiers caractères pour l'année\n",
    "df_objets['Mois'] = df_objets['Date'].str[5:7]         # Les caractères 5 à 6 pour le mois\n",
    "df_objets['Jour'] = df_objets['Date'].str[8:10]        # Les caractères 8 à 9 pour le jour\n",
    "df_objets['Heure'] = df_objets['Date'].str[11:19]         # Les caractères 11 à 18 pour l'heure:minute:seconde\n",
    "df_objets['Décalage_Horaire'] = df_objets['Date'].str[-6:]  # Les 6 derniers caractères pour le décalage horaire\n",
    "\n",
    "# Affichage des premières lignes pour vérifier les nouvelles colonnes\n",
    "print(df_objets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8db1ef38-7c2f-4717-bbd0-b6a24c445c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+02:00' '+01:00']\n"
     ]
    }
   ],
   "source": [
    "print(df_objets['Décalage_Horaire'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c909b87d-2db4-4b31-b212-8e484c0eb08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562419\n",
      "344857\n"
     ]
    }
   ],
   "source": [
    "print(df_objets['Décalage_Horaire'].value_counts().get('+02:00', 0))\n",
    "print(df_objets['Décalage_Horaire'].value_counts().get('+01:00', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6b749714-e614-46c3-8384-01cb9a3a3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date Date et heure de restitution                Gare  \\\n",
      "0  2018-08-16T08:46:55+02:00    2018-08-16T10:52:08+02:00              Nantes   \n",
      "1  2018-08-16T08:50:34+02:00                          NaN               Redon   \n",
      "2  2018-08-16T09:11:18+02:00                          NaN             Béziers   \n",
      "3  2018-08-16T09:19:47+02:00                          NaN              Rennes   \n",
      "4  2018-08-16T09:23:18+02:00                          NaN  Paris Montparnasse   \n",
      "\n",
      "                                Nature d'objets  \\\n",
      "0                            Téléphone portable   \n",
      "1                           Casque (vélo, moto)   \n",
      "2         Sac d'enseigne (plastique, papier, …)   \n",
      "3  Manteau, veste, blazer, parka, blouson, cape   \n",
      "4                                     Sac à dos   \n",
      "\n",
      "                                       Type d'objets Type d'enregistrement  \\\n",
      "0  Appareils électroniques, informatiques, appare...        Objet restitué   \n",
      "1           Vélos, trottinettes, accessoires 2 roues          Objet trouvé   \n",
      "2                Bagagerie: sacs, valises, cartables          Objet trouvé   \n",
      "3                              Vêtements, chaussures          Objet trouvé   \n",
      "4                Bagagerie: sacs, valises, cartables          Objet trouvé   \n",
      "\n",
      "  Année Mois Jour     Heure Décalage_Horaire RAnnée RMois RJour    RHeure  \\\n",
      "0  2018   08   16  08:46:55           +02:00   2018    08    16  10:52:08   \n",
      "1  2018   08   16  08:50:34           +02:00    NaN   NaN   NaN       NaN   \n",
      "2  2018   08   16  09:11:18           +02:00    NaN   NaN   NaN       NaN   \n",
      "3  2018   08   16  09:19:47           +02:00    NaN   NaN   NaN       NaN   \n",
      "4  2018   08   16  09:23:18           +02:00    NaN   NaN   NaN       NaN   \n",
      "\n",
      "  RDécalage_Horaire  \n",
      "0            +02:00  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n"
     ]
    }
   ],
   "source": [
    "# Extraire l'année, le mois, le jour, l'heure et le décalage horaire de 'Date et heure de restitution'\n",
    "df_objets['RAnnée'] = df_objets['Date et heure de restitution'].str[:4]            # Les 4 premiers caractères pour l'année\n",
    "df_objets['RMois'] = df_objets['Date et heure de restitution'].str[5:7]            # Les caractères 5 à 6 pour le mois\n",
    "df_objets['RJour'] = df_objets['Date et heure de restitution'].str[8:10]           # Les caractères 8 à 9 pour le jour\n",
    "df_objets['RHeure'] = df_objets['Date et heure de restitution'].str[11:19]         # Les caractères 11 à 18 pour l'heure:minute:seconde\n",
    "df_objets['RDécalage_Horaire'] = df_objets['Date et heure de restitution'].str[-6:]  # Les 6 derniers caractères pour le décalage horaire\n",
    "\n",
    "# Affichage des premières lignes pour vérifier les nouvelles colonnes\n",
    "print(df_objets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "729a2600-8e3e-4754-8744-5550163b9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toutes les valeurs valides de 'Date et heure de restitution' sont après 'Date'.\n"
     ]
    }
   ],
   "source": [
    "# Convertir les colonnes 'Date' et 'Date et heure de restitution' au format datetime avec UTC\n",
    "df_objets['Date'] = pd.to_datetime(df_objets['Date'], utc=True)\n",
    "df_objets['Date et heure de restitution'] = pd.to_datetime(df_objets['Date et heure de restitution'], utc=True)\n",
    "\n",
    "# Filtrer les lignes où 'Date et heure de restitution' n'est pas NaT\n",
    "valid_dates = df_objets['Date et heure de restitution'].notna()\n",
    "\n",
    "# Vérifier que 'Date et heure de restitution' est après 'Date' uniquement pour les lignes valides\n",
    "date_check = df_objets.loc[valid_dates, 'Date et heure de restitution'] > df_objets.loc[valid_dates, 'Date']\n",
    "\n",
    "# Afficher le résultat\n",
    "if date_check.all():\n",
    "    print(\"Toutes les valeurs valides de 'Date et heure de restitution' sont après 'Date'.\")\n",
    "else:\n",
    "    print(\"Certaines valeurs valides de 'Date et heure de restitution' ne sont pas après 'Date'.\")\n",
    "    print(df_objets.loc[valid_dates & ~date_check, ['Date', 'Date et heure de restitution']])  # Afficher les lignes concernées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bbda1013-f0b8-449e-8363-0f27bb1bdbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couples (jour, mois) avec décalage horaire +01:00 triés : [['01', '01'], ['02', '01'], ['03', '01'], ['04', '01'], ['05', '01'], ['06', '01'], ['07', '01'], ['08', '01'], ['09', '01'], ['10', '01'], ['11', '01'], ['12', '01'], ['13', '01'], ['14', '01'], ['15', '01'], ['16', '01'], ['17', '01'], ['18', '01'], ['19', '01'], ['20', '01'], ['21', '01'], ['22', '01'], ['23', '01'], ['24', '01'], ['25', '01'], ['26', '01'], ['27', '01'], ['28', '01'], ['29', '01'], ['30', '01'], ['31', '01'], ['01', '02'], ['02', '02'], ['03', '02'], ['04', '02'], ['05', '02'], ['06', '02'], ['07', '02'], ['08', '02'], ['09', '02'], ['10', '02'], ['11', '02'], ['12', '02'], ['13', '02'], ['14', '02'], ['15', '02'], ['16', '02'], ['17', '02'], ['18', '02'], ['19', '02'], ['20', '02'], ['21', '02'], ['22', '02'], ['23', '02'], ['24', '02'], ['25', '02'], ['26', '02'], ['27', '02'], ['28', '02'], ['29', '02'], ['01', '03'], ['02', '03'], ['03', '03'], ['04', '03'], ['05', '03'], ['06', '03'], ['07', '03'], ['08', '03'], ['09', '03'], ['10', '03'], ['11', '03'], ['12', '03'], ['13', '03'], ['14', '03'], ['15', '03'], ['16', '03'], ['17', '03'], ['18', '03'], ['19', '03'], ['20', '03'], ['21', '03'], ['22', '03'], ['23', '03'], ['24', '03'], ['25', '03'], ['26', '03'], ['27', '03'], ['28', '03'], ['29', '03'], ['30', '03'], ['25', '10'], ['26', '10'], ['27', '10'], ['28', '10'], ['29', '10'], ['30', '10'], ['31', '10'], ['01', '11'], ['02', '11'], ['03', '11'], ['04', '11'], ['05', '11'], ['06', '11'], ['07', '11'], ['08', '11'], ['09', '11'], ['10', '11'], ['11', '11'], ['12', '11'], ['13', '11'], ['14', '11'], ['15', '11'], ['16', '11'], ['17', '11'], ['18', '11'], ['19', '11'], ['20', '11'], ['21', '11'], ['22', '11'], ['23', '11'], ['24', '11'], ['25', '11'], ['26', '11'], ['27', '11'], ['28', '11'], ['29', '11'], ['30', '11'], ['01', '12'], ['02', '12'], ['03', '12'], ['04', '12'], ['05', '12'], ['06', '12'], ['07', '12'], ['08', '12'], ['09', '12'], ['10', '12'], ['11', '12'], ['12', '12'], ['13', '12'], ['14', '12'], ['15', '12'], ['16', '12'], ['17', '12'], ['18', '12'], ['19', '12'], ['20', '12'], ['21', '12'], ['22', '12'], ['23', '12'], ['24', '12'], ['25', '12'], ['26', '12'], ['27', '12'], ['28', '12'], ['29', '12'], ['30', '12'], ['31', '12']]\n",
      "Couples (jour, mois) avec décalage horaire +02:00 triés : [['25', '03'], ['26', '03'], ['27', '03'], ['28', '03'], ['29', '03'], ['30', '03'], ['31', '03'], ['01', '04'], ['02', '04'], ['03', '04'], ['04', '04'], ['05', '04'], ['06', '04'], ['07', '04'], ['08', '04'], ['09', '04'], ['10', '04'], ['11', '04'], ['12', '04'], ['13', '04'], ['14', '04'], ['15', '04'], ['16', '04'], ['17', '04'], ['18', '04'], ['19', '04'], ['20', '04'], ['21', '04'], ['22', '04'], ['23', '04'], ['24', '04'], ['25', '04'], ['26', '04'], ['27', '04'], ['28', '04'], ['29', '04'], ['30', '04'], ['01', '05'], ['02', '05'], ['03', '05'], ['04', '05'], ['05', '05'], ['06', '05'], ['07', '05'], ['08', '05'], ['09', '05'], ['10', '05'], ['11', '05'], ['12', '05'], ['13', '05'], ['14', '05'], ['15', '05'], ['16', '05'], ['17', '05'], ['18', '05'], ['19', '05'], ['20', '05'], ['21', '05'], ['22', '05'], ['23', '05'], ['24', '05'], ['25', '05'], ['26', '05'], ['27', '05'], ['28', '05'], ['29', '05'], ['30', '05'], ['31', '05'], ['01', '06'], ['02', '06'], ['03', '06'], ['04', '06'], ['05', '06'], ['06', '06'], ['07', '06'], ['08', '06'], ['09', '06'], ['10', '06'], ['11', '06'], ['12', '06'], ['13', '06'], ['14', '06'], ['15', '06'], ['16', '06'], ['17', '06'], ['18', '06'], ['19', '06'], ['20', '06'], ['21', '06'], ['22', '06'], ['23', '06'], ['24', '06'], ['25', '06'], ['26', '06'], ['27', '06'], ['28', '06'], ['29', '06'], ['30', '06'], ['01', '07'], ['02', '07'], ['03', '07'], ['04', '07'], ['05', '07'], ['06', '07'], ['07', '07'], ['08', '07'], ['09', '07'], ['10', '07'], ['11', '07'], ['12', '07'], ['13', '07'], ['14', '07'], ['15', '07'], ['16', '07'], ['17', '07'], ['18', '07'], ['19', '07'], ['20', '07'], ['21', '07'], ['22', '07'], ['23', '07'], ['24', '07'], ['25', '07'], ['26', '07'], ['27', '07'], ['28', '07'], ['29', '07'], ['30', '07'], ['31', '07'], ['01', '08'], ['02', '08'], ['03', '08'], ['04', '08'], ['05', '08'], ['06', '08'], ['07', '08'], ['08', '08'], ['09', '08'], ['10', '08'], ['11', '08'], ['12', '08'], ['13', '08'], ['14', '08'], ['15', '08'], ['16', '08'], ['17', '08'], ['18', '08'], ['19', '08'], ['20', '08'], ['21', '08'], ['22', '08'], ['23', '08'], ['24', '08'], ['25', '08'], ['26', '08'], ['27', '08'], ['28', '08'], ['29', '08'], ['30', '08'], ['31', '08'], ['01', '09'], ['02', '09'], ['03', '09'], ['04', '09'], ['05', '09'], ['06', '09'], ['07', '09'], ['08', '09'], ['09', '09'], ['10', '09'], ['11', '09'], ['12', '09'], ['13', '09'], ['14', '09'], ['15', '09'], ['16', '09'], ['17', '09'], ['18', '09'], ['19', '09'], ['20', '09'], ['21', '09'], ['22', '09'], ['23', '09'], ['24', '09'], ['25', '09'], ['26', '09'], ['27', '09'], ['28', '09'], ['29', '09'], ['30', '09'], ['01', '10'], ['02', '10'], ['03', '10'], ['04', '10'], ['05', '10'], ['06', '10'], ['07', '10'], ['08', '10'], ['09', '10'], ['10', '10'], ['11', '10'], ['12', '10'], ['13', '10'], ['14', '10'], ['15', '10'], ['16', '10'], ['17', '10'], ['18', '10'], ['19', '10'], ['20', '10'], ['21', '10'], ['22', '10'], ['23', '10'], ['24', '10'], ['25', '10'], ['26', '10'], ['27', '10'], ['28', '10'], ['29', '10'], ['30', '10']]\n"
     ]
    }
   ],
   "source": [
    "# Trier les couples (jour, mois) pour chaque décalage horaire par mois, puis par jour\n",
    "couples_plus_01_sorted = (\n",
    "    df_objets[df_objets['Décalage_Horaire'] == '+01:00'][['Jour', 'Mois']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=['Mois', 'Jour'])\n",
    "    .values.tolist()\n",
    ")\n",
    "\n",
    "couples_plus_02_sorted = (\n",
    "    df_objets[df_objets['Décalage_Horaire'] == '+02:00'][['Jour', 'Mois']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=['Mois', 'Jour'])\n",
    "    .values.tolist()\n",
    ")\n",
    "\n",
    "print(\"Couples (jour, mois) avec décalage horaire +01:00 triés :\", couples_plus_01_sorted)\n",
    "print(\"Couples (jour, mois) avec décalage horaire +02:00 triés :\", couples_plus_02_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "703b68e8-e375-49e5-9d1c-a9b5db1981e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couples uniques de 'Nature d'objets' et 'Type d'objets' (triés par 'Nature d'objets') :\n",
      "[('Autres divers', 'Divers'), ('Autres divers', \"Articles d'enfants, de puériculture\"), ('Autres divers', \"Pièces d'identités et papiers personnels\"), ('Autres vêtements', 'Vêtements, chaussures'), ('Autres vêtements', 'Optique'), (\"Carte d'abonnement\", \"Pièces d'identités et papiers personnels\"), (\"Carte d'abonnement\", 'Porte-monnaie / portefeuille, argent, titres'), ('Chéquier', 'Porte-monnaie / portefeuille, argent, titres'), ('Chéquier', 'Bagagerie: sacs, valises, cartables'), ('Manteau, veste, blazer, parka, blouson, cape', 'Vêtements, chaussures'), ('Manteau, veste, blazer, parka, blouson, cape', 'Appareils électroniques, informatiques, appareils photo'), ('Porte-monnaie, portefeuille', 'Porte-monnaie / portefeuille, argent, titres'), ('Porte-monnaie, portefeuille', \"Pièces d'identités et papiers personnels\")]\n"
     ]
    }
   ],
   "source": [
    "# Récupérer les colonnes 'Nature d\\'objets' et 'Type d\\'objets'\n",
    "df_nature_type = df_objets[['Nature d\\'objets', 'Type d\\'objets']]\n",
    "\n",
    "# Supprimer les doublons\n",
    "unique_pairs = df_nature_type.drop_duplicates()\n",
    "\n",
    "# Vérifier si chaque 'Nature d\\'objets' est associée à un seul 'Type d\\'objets'\n",
    "nature_to_type = unique_pairs.groupby('Nature d\\'objets')['Type d\\'objets'].nunique()\n",
    "\n",
    "# Sélectionner les 'Nature d\\'objets' ayant plus d'un type d'objets associé\n",
    "natures_with_multiple_types = nature_to_type[nature_to_type > 1].index\n",
    "\n",
    "# Filtrer les couples uniques pour les 'Nature d\\'objets' ayant plus d'un type d'objets associé\n",
    "multiple_type_pairs = unique_pairs[unique_pairs['Nature d\\'objets'].isin(natures_with_multiple_types)]\n",
    "\n",
    "# Trier les résultats par \"Nature d'objets\" dans l'ordre alphabétique\n",
    "sorted_pairs = multiple_type_pairs.sort_values(by='Nature d\\'objets')\n",
    "\n",
    "# Convertir en liste de tuples\n",
    "result = list(sorted_pairs.itertuples(index=False, name=None))\n",
    "\n",
    "# Afficher la liste de tuples\n",
    "print(\"Couples uniques de 'Nature d'objets' et 'Type d'objets' (triés par 'Nature d'objets') :\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad439a77-a0e2-4de0-a988-44ac9036c68f",
   "metadata": {},
   "source": [
    "Il y existe des natures d'objets qui ont plusieurs types de d'objets associé. Type d'objets peut-il est considéré comme étant une dimension en plus ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eab4e69-79f0-4bd1-8e2c-33e1dd7bd40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1079 entries, 11561 to 907275\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Date                          1079 non-null   object \n",
      " 1   Date et heure de restitution  186 non-null    object \n",
      " 2   Gare                          0 non-null      object \n",
      " 3   Code UIC                      0 non-null      float64\n",
      " 4   Nature d'objets               1079 non-null   object \n",
      " 5   Type d'objets                 1079 non-null   object \n",
      " 6   Type d'enregistrement         1079 non-null   object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 67.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_missing_gare = df_objets[df_objets['Gare'].isna()]\n",
    "print(df_missing_gare.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fa04186a-394c-48fc-b364-b47ae76b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs NaN dans la colonne 'Gare' par 'Gare Inconnue'\n",
    "df_objets['Gare'] = df_objets['Gare'].fillna(\"Gare Inconnue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10172055-a4cc-48ca-8a33-1d5cfddda80a",
   "metadata": {},
   "source": [
    "## Changement de sujet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "83f87c59-2ea2-4cbf-9a74-af6a31f95d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Date Date et heure de restitution  \\\n",
      "769954 2015-06-15 09:35:44+00:00                          NaT   \n",
      "539855 2023-11-13 09:07:26+00:00    2023-12-05 14:53:38+00:00   \n",
      "14585  2019-08-21 13:48:14+00:00    2019-08-21 14:17:49+00:00   \n",
      "165670 2022-12-16 13:23:05+00:00                          NaT   \n",
      "293243 2022-01-26 19:49:35+00:00    2022-03-30 08:47:16+00:00   \n",
      "\n",
      "                      Gare                        Nature d'objets  \\\n",
      "769954  Paris Saint-Lazare                       Pull over, gilet   \n",
      "539855       Lyon Perrache                              Sac à dos   \n",
      "14585            Paris Est  Sac d'enseigne (plastique, papier, …)   \n",
      "165670  Paris Montparnasse        Autre pièce ou papier personnel   \n",
      "293243  Paris Gare de Lyon              Valise, sac sur roulettes   \n",
      "\n",
      "                                   Type d'objets Type d'enregistrement Année  \\\n",
      "769954                     Vêtements, chaussures          Objet trouvé  2015   \n",
      "539855       Bagagerie: sacs, valises, cartables        Objet restitué  2023   \n",
      "14585        Bagagerie: sacs, valises, cartables        Objet restitué  2019   \n",
      "165670  Pièces d'identités et papiers personnels          Objet trouvé  2022   \n",
      "293243       Bagagerie: sacs, valises, cartables        Objet restitué  2022   \n",
      "\n",
      "       Mois Jour     Heure Décalage_Horaire RAnnée RMois RJour    RHeure  \\\n",
      "769954   06   15  11:35:44           +02:00    NaN   NaN   NaN       NaN   \n",
      "539855   11   13  10:07:26           +01:00   2023    12    05  15:53:38   \n",
      "14585    08   21  15:48:14           +02:00   2019    08    21  16:17:49   \n",
      "165670   12   16  14:23:05           +01:00    NaN   NaN   NaN       NaN   \n",
      "293243   01   26  20:49:35           +01:00   2022    03    30  10:47:16   \n",
      "\n",
      "       RDécalage_Horaire  \n",
      "769954               NaN  \n",
      "539855            +01:00  \n",
      "14585             +02:00  \n",
      "165670               NaN  \n",
      "293243            +02:00  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 769954 to 323046\n",
      "Data columns (total 16 columns):\n",
      " #   Column                        Non-Null Count   Dtype              \n",
      "---  ------                        --------------   -----              \n",
      " 0   Date                          100000 non-null  datetime64[ns, UTC]\n",
      " 1   Date et heure de restitution  38712 non-null   datetime64[ns, UTC]\n",
      " 2   Gare                          100000 non-null  object             \n",
      " 3   Nature d'objets               100000 non-null  object             \n",
      " 4   Type d'objets                 100000 non-null  object             \n",
      " 5   Type d'enregistrement         100000 non-null  object             \n",
      " 6   Année                         100000 non-null  object             \n",
      " 7   Mois                          100000 non-null  object             \n",
      " 8   Jour                          100000 non-null  object             \n",
      " 9   Heure                         100000 non-null  object             \n",
      " 10  Décalage_Horaire              100000 non-null  object             \n",
      " 11  RAnnée                        38712 non-null   object             \n",
      " 12  RMois                         38712 non-null   object             \n",
      " 13  RJour                         38712 non-null   object             \n",
      " 14  RHeure                        38712 non-null   object             \n",
      " 15  RDécalage_Horaire             38712 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](2), object(14)\n",
      "memory usage: 13.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sample_df = df_objets.sample(n=100000, random_state=42)\n",
    "\n",
    "# Afficher un aperçu de l'échantillon\n",
    "print(sample_df.head())\n",
    "print(sample_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4fb8d4f6-f325-44fb-93d0-6bcfeda770bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'observations avec des couples Décalage_Horaire et RDécalage_Horaire différents (sans NaN) : 19467\n",
      "Exemple des premières différences trouvées :\n",
      "    Décalage_Horaire RDécalage_Horaire\n",
      "209           +02:00            +01:00\n",
      "252           +02:00            +01:00\n",
      "394           +02:00            +01:00\n",
      "592           +02:00            +01:00\n",
      "681           +02:00            +01:00\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les observations sans NaN dans 'Décalage_Horaire' et 'RDécalage_Horaire' puis comparer\n",
    "different_timezone_pairs = df_objets[\n",
    "    df_objets['Décalage_Horaire'].notna() &\n",
    "    df_objets['RDécalage_Horaire'].notna() &\n",
    "    (df_objets['Décalage_Horaire'] != df_objets['RDécalage_Horaire'])\n",
    "]\n",
    "\n",
    "# Compter et afficher les résultats\n",
    "count_different_timezone_pairs = different_timezone_pairs.shape[0]\n",
    "different_timezone_pairs_head = different_timezone_pairs[['Décalage_Horaire', 'RDécalage_Horaire']].head()\n",
    "\n",
    "print(\"Nombre d'observations avec des couples Décalage_Horaire et RDécalage_Horaire différents (sans NaN) :\", count_different_timezone_pairs)\n",
    "print(\"Exemple des premières différences trouvées :\")\n",
    "print(different_timezone_pairs_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8e818-869c-49c9-8414-0add2397fa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
